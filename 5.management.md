# Test Management

5.1 Test Organization
5.2 Test planning and Estimation
5.3 Test Monitoring and Control
5.4 Configuration Management
5.5 Risk and Testing
5.6 Defect Management

## 5.1 Test Organization

### Test organization and independence

-A certain degree of independance often makes the tester more effective at finding defects and failures.  Degrees of independince in testing include the following  (from low level of independence to high level):
    --No independent testers; the only form of testing available is developers testing their own code
    -- independent developers or testers within the development teams or project team;  this could be develoeprs testing their colleagues products
    --independent test team or group within the organization, reporting to project management or executive management
    --indepedent testers from the business organization or user community or with specializations in specific test types such as usability, security, performance, regulatory/compliance or portability
    --indpendent testers external to the organization, either workintg on-site (insourcing) or offsite (outsourcing)
-Potential benefits of test independence include: 
    --Independent testers are likely to recognize different kinds of failures compared to developers bc of their different backgroundss, technical perspectives and biases
    -an independent tester can verify, challenge or disprove assumptions made by stakeholders during specification  and implementation of the system
-Potential Drawbacks of test independence include:
    --Isolation from the development team, leading to a lack of collaboration, delays in providing feedback to the development team or an adversarial relationship with the developement team
    -developers may lose a sense of responsibility for quality
    -independent testers may be seen as a bottleneck or blamed for delays in release
    -indpendent testers may lack some important information (e.g about the test object)

### Task of a test Manager
-develop or review a test policy and test strategy for the organization
-plan the test activities by considering the context, and understanding the test objectives and risks
-coordinate the test plans with project managers, product owners and others
-share testing perspectives with other project activities such as integration planning
-initiate the analysis, design, implementation and execution of tests, monitor test progress and results and check the status of exit criteria
-prepare and deliver test progress reports and test summary reports based on the information gathered. 
-support setting up the defect management system and adequate configuration management of testware
-introduce suitable metricsd for measuring test progress and evaluationg the quality of teh testing and the product
-support the selection and implementation fo tools to support the test process
-decide about the implementation of test environments
-develop the skills & careers of testers ( e.g through training plans, performance evaluations coaching etc)

## 5.2 Test Planning and Estimation

### Test Planning
-Determine the scope & risks identifying the objective of testing
-defining the overall apprach of testing
-integrating and coordinating testing activates into software lifecycle
-making decisions about what to test
-scheduling test activities, assigning resources for the activities
-assigning resources for different activities defined
-selecting metrics for monitoring and controlling
-defining entry & exit criteria

### Test Strategy, Test Approach
-A test Strategy provides a generalized description of the test process, usually at the product or organization level.  Common types of test strategies include: 
    --analytical ( risk based testing )
    --model-based
    --Methodical ( predefined of taxonomy based)
    --process compliant (standard compliant)
    --directived (consultative)
    --regression averse
    --reactive (ongoing)

### Entry criteria
-typical entry criteria include:
    --availability of testable requirements and/or models ( eg when following a model based testing strategy)
    --Availability of test items tthat have met the exit criteria for any prior test levels
    --availability of necessary test tools
     --availability of test data and other necessary resources

### Test Estimation
-There are a number of estimation techniques used to determine the effort required for adequate testing.  Two of the most commonly used techniques are:
    --The metrics based technique: estimating the test effort base don metrics of former similar projects, or based on typical values
    --The expert based technique:  Estimating the test effort based on the experience of the owners of the testing tasks or by experts
-The testing effort may depend on a number of factors
    --Characteristics of the product
    --characteristics of the development process
    --people characteristics
    --test results

### Test Execution Schedule
-Once the various test cases and test procedures are produced and assembled into test suites, the test su ites can be arranged in a test execution schedule that defines the order in which they are to be run
-the test execution schedule should take into account such facotors as prioritization, dependencies, confirmation tests, regression tests and the most efficient sequence for executing the tests. 
-Ideally test cases would be ordered to run based on their priority levels, usually by execujting the test cases with the highest priority first 
-however this practice may not work if the test cases have dependencies or the features being tested have dependencies
-if a test case with higher piority is dependent on a test case with lower priority, the lower priority test case must be executed first. 



## 5.3 Test Monitoring and Control

-The purpose of test monitoring is to gather information and provide feedback and visibility about test activities. 
-Information to be monitored may be collected manually or automatically and should be used to assess test progress and to measure wheather teh test exit criteria  or the testing tasks associated with an agile projects definition of done, are satisfied such as meeting the targets for coverage of product risks, requirements ore acceptance criteria

### Metrics used in testing
-Common test metrics include
    --Percentage of planned work done in test case preparation ( or percentage of planned test cases implemented)
    --Percentage of planned work done in test environment preparation
    --test case execution (eg number of test cases run/not run, test cases passed/failed and or test conditions passed/failed)
    --Defect information (eg, defect density, defects found and fixed, failure rate and confirmation test results)
    --test coverage of requirements, user stories, acceptance criteria, risks, or code
    --task completion, resource allocation and usage and effort
    --costs of testing, including the cost compared to the benefit of finding the next defect or the cost compared to the benefit of running the next test

### Test control
-test control describes any guiding or corrective actions taken as a result of information and metrics gathered and ( possibly) reported
-actions may cover any test activity and may affect any other software lifecycle activity
-examples of test control actions include
    --re-prioritizing tests when an identified risk occurs (e.g software delivered late)
    --changing the test schedule due to availability or unavailability of a test environment or other resource
    --re-evaluatign whether a test item meets an entry or exit criterion due to rework

### Test Reports
-Typical test progress reports and test summary reports may include:
    --summary of testing performed
    --information on what occurred during a test period
    --deviations from plan, including deviations in schedule, duration or effort of test activities
    --status of testing and product qualify with respect to the exit criteria or definition of done
    --factors that have blocked or continue to block progress
    --metrics of defects, test cases, test coverage, activity progress and resource consumption
    --residual risks
    --reusable test work products produced



## 5.4 Configuration Management

-The purpose of configuration management is to establish and maintain the integrity of the component or system, the testware, and their relationshop to one another through the project and product lifecycle
-To properly support testing, configuration management may involve ensuring the following
    --All test items are  uniquely identified, version controlled, tracked for changes and related to each other
    --ALl items of testware are uniquely identified, version controlled, tracked for changes, related to each other and related to versions of the test items so that traceability can be maintained throughout the test process
    --all identified documents and software items are referenced unambiguously in test documentation
-during test planning, configuration management procedures and infrastructure tools should be identified and implemented. 



## 5.5 Risk and Testing

### Risks
-Risk involves the possibility of an event in th4e future which has negative consequences
-The level of risk is determined by the 
    --likelihood of the event
    --impact ( the harm ) from that event
-In a risk based approach, the result of product risk analysis are used to :
    --Determine the test techniques to be employed
    --Determine the particular levels and types of testing to be performed (eg security testing, accessibility testing)
    --Determine the extent of testing to be carried out
    --prioritize testing in an attempt to find the critical defects as early as possible
    --determine whether any activities in addition to testing could be employed to reduce risk (eg providing training to inexperienced designers)

### Project Risks
-Project risk involves situations that, should they occur, may have a negative effect on a projects ability to achieve its objectives.  Examples of project risks include:
--Project Issues:
    --delays may occur in deliver, task completion, or satisfaction of exit criiteria or definition of done
    --inaccurate estimates, reallocation of funds to higher priority projects, or general cost cutting across the organization may result in inaddequate funding
    -Late changes may result in substantial rework
--Organizational issues:
    --Skills, training, and staff may not be sufficient
    --personnel issues may cause conflict and problem
    --users, business staff or subject matter experts may not be available due to conflicting business priorities
--Political Issues
    --Testers may not communicate their needs and/or the test results inadequately
    --developers and/or testers may fail to follow up on information found in testing and reviews (not improving development and testing practices)
    --There may be an improper attitude toward or expectatios of testing ( not appreciating the value of finding defects during testing)
--Technical issues
    --Requirements may not be defined well enough
    --requirements may not be met, given existing constraints
    --test environment may not be ready on time
    --data conversion, migration planning and their tool support may be late
    --poor defect management and similar problems may result in accumulated defects and other technical debt
--supplier issues
    --a third party may fail to deliver a necessary product or service or go bankrupt
    --contractual issues may cause problems to the project

### Product Risks
-Product risk involves the possibility that a work product ( a specification, component, system or test) may fail to satisfiy the legitmate needs of its users and/or stakeholders.  When the product risks are associated with specific quality characteristics of a product ( functional suitability, reliability, performance efficiency, usability, security, compatibility, maintainability and portability) product risks are also called quality risks
-Examples of product risks include
    --software might not perform its intended functions according to the specification
    --software might not perform its intended functions according to the user, customer and/or stakeholder needs
    --a system architecture may not adequately support some non-functional requirement
    --a particular computation may be performed incorrectly in some circumstances
    --poor data integrity and quality
    --response times may be inadequate for a high performance transaction processing system


## 5.6 Defect Management
--------------------->---------------------------------------> 
                                                                    \/
NEW                                                                 |
                                                                    |
                            REJECTED                                |
                                                                    |
                            DEFERRED                                |
OPEN        ASSIGNED                        VERIFY                  |
                            RESOLVED                    CLOSED      \/
                                            REOPEN

NEW
OPEN or CLOSED based on evaluation
If open 
ASSIGNED
then
DEFERRED, RESOLVED or REJECTED by the dev
if RESOLVED
then VERIFY by tester
then either 
CLOSED OR REOPEN

### Objectives of a Defect Report
-provide developers and other parties with infomration about any adverse event that occurred, to enable them to identify specific effects, to isolate the problem with minimal reproducing test and to correct the potential defects as needed or othersie resolve the problem
-provide test managers a means of tracking the quality  of the work product and the impact on the testing ( eg if a lot of defects are reported the testers will have spent a lot of time reporting them instead of running tests,and there will be more confirmation testing needed).
-provide ideas for development of test process improvement

### A typical defect report includes
-a unique identifier
-a title and short summary of the defect being reported.  Date of the defect report, issuing organization and author
-identification of the test item (configuration of item being tested) and environment
-the development lifecycle phases in wich the defect was observed
-a description of the defect to enable reproduction and resolution , including logs, database dump screenshots or recordings ( if found during execution )
-expected and actual results
-scope or degree of impact ( severity) of the defecdt on the interest of the stakeholders
-Urgency/priority to fix
-state of the defect report (open, deferred, duplicate, waiting to be fixed, awaiting confirmation testing, reopened, closed)
-conclusions, recommendations and approvals
-global issues such as other areas that may be affected by a change resulting from the defect
-change history such as the sequence of actions taken by a project team members with respect to the defect to isolate, repair, and confirm it as fixed
-references including the test case that revealed the problem






